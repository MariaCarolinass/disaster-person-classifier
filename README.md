# Urban Disaster Monitor: Detec√ß√£o e Classifica√ß√£o de Pessoas em Cen√°rios de Desastre

![YOLOv8](https://img.shields.io/badge/YOLOv8-ultralytics-red) [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ids-NQ6EfzGgfK41BWvkIXxOkUomexo0?usp=sharing) ![License](https://img.shields.io/badge/license-MIT-blue.svg) ![Status](https://img.shields.io/badge/status-MVP%20Ready-brightgreen)

Sistema inteligente de vis√£o computacional para **detec√ß√£o e classifica√ß√£o de civis e socorristas** em cen√°rios de desastre urbano, utilizando **YOLOv8**.

<p align="center">
  <img src="static/images/capa1.webp" alt="Capa do projeto" width="400"/> 
  <img src="static/images/capa2.png" alt="Capa do projeto" width="400"/>
</p>

`YOLOv8` ‚Ä¢ `vis√£o computacional` ‚Ä¢ `desastres naturais` ‚Ä¢ `detec√ß√£o de pessoas` ‚Ä¢ `cidades inteligentes` ‚Ä¢ `socorristas` ‚Ä¢ `gradio` ‚Ä¢ `imagem e v√≠deo`

---

## üìë Sum√°rio

- [Funcionalidades](#-funcionalidades)
- [Introdu√ß√£o](#-introdu√ß√£o)
- [Dataset](#-dataset)
- [Metodologia](#-metodologia)
- [Resultados e discuss√µes](#-resultados-e-discuss√µes)
  - [Avalia√ß√£o em Imagens Est√°ticas](#avalia√ß√£o-em-imagens-est√°ticas)
  - [Avalia√ß√£o em V√≠deo](#Ô∏è-avalia√ß√£o-em-v√≠deo)
  - [Gr√°ficos e Visualiza√ß√µes](#-gr√°ficos-e-visualiza√ß√µes)
  - [Discuss√µes](#discuss√µes)
- [Conclus√µes](#-conclus√µes)
- [Interface Interativa](#-interface-interativa)
- [Tecnologias](#-tecnologias)
- [Equipe do Projeto](#-equipe-do-projeto)
- [Reposit√≥rio de Dados](#-reposit√≥rio-de-dados)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Como Executar](#-como-executar)
- [Refer√™ncias Bibliogr√°ficas](#-refer√™ncias-bibliogr√°ficas)

---

## üîç Funcionalidades

- Detec√ß√£o de pessoas em **imagens** e **v√≠deos**
- Diferencia√ß√£o entre **civis** e **socorristas**
- Treinamento com **YOLOv8**
- Interface interativa via **Gradio**
- Visualiza√ß√£o de **m√©tricas e bounding boxes**

---

## üìå Introdu√ß√£o

Eventos de **desastre urbano**, como colapsos estruturais, inunda√ß√µes e deslizamentos de terra, imp√µem desafios significativos √†s opera√ß√µes de resposta e resgate. A capacidade de **identificar e categorizar rapidamente** indiv√≠duos como civis ou socorristas em tempo real √© crucial para a otimiza√ß√£o da aloca√ß√£o de recursos e a minimiza√ß√£o de fatalidades. Imagens coletadas por ve√≠culos a√©reos n√£o tripulados (VANTs), sistemas de vigil√¢ncia e dispositivos m√≥veis representam uma fonte de dados valiosa para esta finalidade.

Este projeto prop√µe o desenvolvimento de um **sistema inteligente de vis√£o computacional** para a **detec√ß√£o e classifica√ß√£o discriminativa de indiv√≠duos (civis e socorristas)** em ambientes impactados por desastres urbanos. Utilizando a arquitetura de **detec√ß√£o de objetos YOLOv8 (You Only Look Once, vers√£o 8)**, o objetivo √© construir uma ferramenta robusta e eficiente capaz de fornecer suporte cr√≠tico a autoridades e equipes de emerg√™ncia durante a fase de resposta a desastres.

Desenvolvido durante a disciplina de Vis√£o Computacional na **Escola de Ci√™ncias e Tecnologia (ECT/UFRN)** em trabalho volunt√°rio para o **Smart Metropolis Lab (SMLab)** do **Instituto Metr√≥pole Digital (IMD/UFRN)**, este trabalho √© parte integrante do **Projeto SPICI (Seguran√ßa P√∫blica Integrada em Cidades Inteligentes)**. O SPICI visa criar uma plataforma inteligente para coleta, processamento e an√°lise de imagens e informa√ß√µes cr√≠ticas em tempo real, contribuindo para a gest√£o de crises e desastres por meio de tecnologias avan√ßadas como vis√£o computacional e intelig√™ncia artificial. O `Urban Disaster Monitor` alinha-se diretamente com os objetivos do SPICI ao fornecer uma ferramenta especializada para a identifica√ß√£o de pessoas em cen√°rios de desastre, agregando valor √† capacidade de resposta e tomada de decis√£o da plataforma.

<a href="https://smlab.imd.ufrn.br/">
  <img src="static/images/smartmetropolislab.png" alt="Logo Smart Metropolis Lab" width="150"/>
</a>

<a href="https://smlab.imd.ufrn.br/projeto-spici/">
  <img src="static/images/spici.png" alt="Logo SPICI" width="100"/>
</a>

<a href="https://imd.ufrn.br/">
  <img src="static/images/imd-logo.7784f1db.webp" alt="Logo IMD/UFRN" width="150"/>
</a>

<a href="https://www.ect.ufrn.br/">
  <img src="static/images/cet.png" alt="Logo IMD/UFRN" width="100"/>
</a>

---

## üìÇ Dataset

- **Total de imagens**: 2403
- **Fontes**: dados reais + imagens sint√©ticas
- **Classes**:
  - `people` (civis)
  - `rescuer` (socorristas com EPI)
- **Anotado via**: [Roboflow](https://roboflow.com)

---

## ‚öôÔ∏è Metodologia

- Arquitetura: `YOLOv8s` e `YOLOv8n`
- Aumento de dados: rota√ß√£o, ru√≠do, brilho, etc.
- Treinamento: 30 e 50 √©pocas comparadas
- M√©tricas principais:
  - `mAP@0.5`, `mAP@0.5:0.95`
  - `Precision`, `Recall`
  - Matriz de confus√£o

### Aquisi√ß√£o e Anota√ß√£o de Dados

Para o treinamento e valida√ß√£o do modelo, foi compilado um **dataset abrangente** composto por diversas imagens que representam cen√°rios t√≠picos de desastres urbanos (e.g., edif√≠cios colapsados, √°reas inundadas). As fontes incluem **reposit√≥rios p√∫blicos de imagens de desastres** e a **gera√ß√£o de imagens sint√©ticas** para complementar a variabilidade do conjunto de dados. Essa abordagem h√≠brida visa mitigar a escassez de dados reais de alta qualidade em ambientes de desastre.

A **anota√ß√£o dos objetos de interesse** est√° sendo realizada na plataforma **Roboflow**, onde as classes prim√°rias de interesse s√£o definidas como:

- `People`: Indiv√≠duos n√£o identificados como membros de equipes de resgate.
- `Rescuer`: Indiv√≠duos equipados ou identific√°veis como parte de equipes de emerg√™ncia (e.g., bombeiros, param√©dicos), frequentemente distingu√≠veis por uniformes, equipamentos de prote√ß√£o individual (EPI) ou posturas operacionais.

### Pr√©-processamento de Dados

As etapas de pr√©-processamento s√£o cruciais para otimizar o desempenho do modelo e garantir sua generaliza√ß√£o:

- **Redimensionamento**: Todas as imagens s√£o escaladas para dimens√µes compat√≠veis com as entradas da arquitetura YOLOv8, balanceando a fidelidade da informa√ß√£o visual com a efici√™ncia computacional.
- **Aumento de Dados (Data Augmentation)**: T√©cnicas de aumento de dados (e.g., rota√ß√£o, transla√ß√£o, espelhamento, ajuste de brilho e contraste, _blurring_ e ru√≠do) s√£o aplicadas para aumentar a robustez do modelo, mitigar o risco de _overfitting_ e compensar potenciais desequil√≠brios entre as classes. A diversifica√ß√£o do dataset atrav√©s do _data augmentation_ √© fundamental para simular a variabilidade das condi√ß√µes de imagem em cen√°rios reais de desastre.
- **Particionamento**: O dataset √© dividido em conjuntos de treino, valida√ß√£o e teste, tipicamente nas propor√ß√µes 70%, 20% e 10%, respectivamente. Essa divis√£o estratificada assegura uma avalia√ß√£o imparcial da capacidade de generaliza√ß√£o do modelo para dados n√£o vistos.

### Arquitetura do Modelo e Ferramentas

- **Modelo de Detec√ß√£o**: **YOLOv8 (You Only Look Once, vers√£o 8)**. Essa arquitetura foi selecionada por sua comprovada efici√™ncia na detec√ß√£o de objetos em tempo real, combinando alta acur√°cia com velocidade de infer√™ncia, caracter√≠sticas essenciais para aplica√ß√µes em cen√°rios emergenciais.
- **Tarefa**: Detec√ß√£o de Objetos (Object Detection).
- **Frameworks e Bibliotecas**: O desenvolvimento e treinamento do modelo s√£o realizados com **Ultralytics YOLOv8**, utilizando **PyTorch** como _backend_ de aprendizado profundo, e **OpenCV** para manipula√ß√£o de imagens e pr√©-processamento de v√≠deo.

### Treinamento e Otimiza√ß√£o do Modelo

O processo de treinamento envolve as seguintes fases, visando a otimiza√ß√£o do desempenho do modelo:

1.  **Convers√£o de Anota√ß√µes**: As anota√ß√µes geradas no formato Roboflow s√£o convertidas para o formato YOLO (`.txt`), que √© o padr√£o de entrada para os modelos da fam√≠lia YOLO.
2.  **Treinamento Iterativo**: O modelo √© treinado utilizando um conjunto de par√¢metros otimizados, incluindo _batch size_, taxa de aprendizado (_learning rate_) e n√∫mero de _epochs_. T√©cnicas de agendamento da taxa de aprendizado (_learning rate schedulers_) e otimizadores (e.g., Adam, SGD) s√£o empregadas para acelerar a converg√™ncia e melhorar a performance.
3.  **Valida√ß√£o Cont√≠nua**: O desempenho do modelo √© monitorado e validado em tempo real durante o treinamento utilizando m√©tricas de desempenho padr√£o da √°rea em um conjunto de valida√ß√£o separado. Isso permite identificar _overfitting_ e ajustar hiperpar√¢metros de forma din√¢mica.
4.  **Testes e Avalia√ß√£o**: Ap√≥s o treinamento, o modelo √© avaliado extensivamente em um conjunto de teste independente, contendo imagens reais e cen√°rios n√£o vistos, para verificar sua capacidade de generaliza√ß√£o e robustez em condi√ß√µes adversas.

---

## üìä Resultados e discuss√µes

### An√°lise de Resultados e M√©tricas

A avalia√ß√£o da efic√°cia do modelo ser√° realizada atrav√©s de um conjunto de m√©tricas quantitativas e qualitativas, fornecendo uma an√°lise abrangente do seu desempenho:

- **M√©tricas de Precis√£o M√©dia (mAP)**:
  - **mAP@0.5**: _Mean Average Precision_ calculada com um _IoU (Intersection over Union)_ _threshold_ de 0.5. Essa m√©trica √© comumente utilizada para avalia√ß√£o r√°pida da precis√£o de detec√ß√£o.
  - **mAP@0.5:0.95**: _Mean Average Precision_ calculada em m√∫ltiplos _thresholds_ de _IoU_, variando de 0.5 a 0.95 em etapas de 0.05. Essa m√©trica oferece uma medida mais robusta e abrangente da acur√°cia de localiza√ß√£o e classifica√ß√£o das detec√ß√µes.
- **Precis√£o (Precision) e Recall (Revoca√ß√£o)**: Avaliados por classe para compreender o desempenho individual na identifica√ß√£o de civis e socorristas, indicando a propor√ß√£o de detec√ß√µes corretas e a capacidade do modelo de encontrar todas as inst√¢ncias relevantes, respectivamente.
- **Matriz de Confus√£o (Confusion Matrix)**: Essencial para visualizar e analisar os tipos de erros (falsos positivos, falsos negativos) cometidos pelo modelo, especialmente a confus√£o entre as classes de interesse, o que √© cr√≠tico em cen√°rios onde a distin√ß√£o entre civis e socorristas √© vital.
- **An√°lise Qualitativa**: Ser√£o apresentadas visualiza√ß√µes das _bounding boxes_ e r√≥tulos de classe sobre as imagens de teste. Essa an√°lise visual permite uma avalia√ß√£o subjetiva da acur√°cia das detec√ß√µes e da capacidade do modelo de lidar com varia√ß√µes de escala, oclus√£o e ilumina√ß√£o.

Os resultados ser√£o detalhadamente apresentados em relat√≥rios t√©cnicos e artigos cient√≠ficos, incluindo gr√°ficos e tabelas estat√≠sticas, acompanhados de uma an√°lise cr√≠tica sobre as limita√ß√µes do modelo e o potencial de aplica√ß√£o em contextos reais de desastre.

### Avalia√ß√£o em Imagens Est√°ticas

Foram realizados testes qualitativos em imagens fora do treinamento de alagamentos para validar a capacidade de detec√ß√£o do modelo YOLOv8 treinado em diferentes regimes de √©pocas (30 e 50).

- Com **30 √©pocas**, o modelo j√° era capaz de detectar pessoas e socorristas com precis√£o razo√°vel.
- Com **50 √©pocas**, observou-se uma melhora clara nas imagens est√°ticas, com menos falsos positivos, corrigindo erros como a identifica√ß√£o incorreta de pessoas em postes ou √°reas de sombra.

**Imagens utilizadas:**

<p align="center">
  <img src="static/images/teste1.webp" alt="Imagem de teste 1" width="400"/>
  <img src="static/images/teste1-saida.webp" alt="Imagem de teste 1 sa√≠da" width="400"/>
</p>

<p align="center">
  <img src="static/images/teste2.webp" alt="Imagem de teste 2" width="400"/>
  <img src="static/images/teste2-saida.webp" alt="Imagem de teste 2 sa√≠da" width="400"/>
</p>

*Fonte das imagens: [BBC Brasil - Alagamentos em SP](https://www.bbc.com/portuguese/articles/cw00d51k5rlo)*

### üéûÔ∏è Avalia√ß√£o em V√≠deo

Um v√≠deo p√∫blico do YouTube foi utilizado para simular um cen√°rio real de desastre urbano:

- O modelo com **30 √©pocas** apresentou maior estabilidade e consist√™ncia ao longo dos quadros.
- O modelo com **50 √©pocas**, embora superior para imagens, teve comportamento err√°tico em v√≠deo, gerando detec√ß√µes flutuantes e menos confi√°veis ‚Äî sugerindo poss√≠vel overfitting ou limita√ß√£o da generaliza√ß√£o temporal.

[V√≠deo utilizado no experimento](https://www.youtube.com/watch?v=QnFwDqzCwRU)

**Observa√ß√£o**: Treinar com mais de 50 √©pocas pode exigir hardware com maior capacidade de mem√≥ria. Durante o experimento, o uso da conta acad√™mica no Google Colab atingiu o limite de mem√≥ria, refor√ßando a necessidade de infraestrutura mais robusta para lidar com sequ√™ncias temporais (v√≠deos).

---

### üìä Gr√°ficos e Visualiza√ß√µes

**1. Gr√°fico de erro durante o treinamento**

30 √©pocas:

![Gr√°fico de erro durante o treinamento - 30 √©pocas](static/graphics/graficoerro30p.png)

50 √©pocas:

![Gr√°fico de erro durante o treinamento - 50 √©pocas](static/graphics/graficoerro50p.png)

**2. Gr√°fico de m√©tricas por √©poca**

![Gr√°fico de m√©tricas por √©poca](static/graphics/graficometricas.png)

**3. Matriz confus√£o**

30 √©pocas:

![Matriz confus√£o - 30 √©pocas](static/matrix/matriz-30ep.png)

50 √©pocas:

![Matriz confus√£o - 50 √©pocas](static/matrix/matriz-50ep.png)

### Discuss√£o

_(Esta se√ß√£o ser√° dedicada √† discuss√£o aprofundada dos resultados obtidos, incluindo a interpreta√ß√£o das m√©tricas de desempenho, a identifica√ß√£o de pontos fortes e fracos do modelo, e a an√°lise de poss√≠veis vieses nos dados. Ser√£o abordadas as limita√ß√µes inerentes √† metodologia, como a sensibilidade a condi√ß√µes de ilumina√ß√£o extremas, oclus√£o parcial ou total dos indiv√≠duos, e a representatividade do dataset. Al√©m disso, ser√£o propostas futuras linhas de pesquisa para aprimoramento do sistema, como a integra√ß√£o de informa√ß√µes contextuais (e.g., sensores multiespectrais), a otimiza√ß√£o para implanta√ß√£o em dispositivos de borda para infer√™ncia em tempo real e a explora√ß√£o de arquiteturas de modelos mais leves. Finalmente, ser√° discutida a integra√ß√£o do `Urban Disaster Monitor` como um m√≥dulo crucial dentro da plataforma SPICI, real√ßando seu impacto potencial na gest√£o inteligente de desastres e na coordena√ß√£o de equipes de resgate.)_

---

## ‚úÖ Conclus√£o

**30 √©pocas** ‚Üí melhor desempenho em v√≠deos, com menos ru√≠do nas detec√ß√µes.

**50 √©pocas** ‚Üí superioridade em imagens est√°ticas, com maior precis√£o espacial.

**Trabalhos futuros:**

- Ajuste de hiperpar√¢metros espec√≠ficos para v√≠deo.
- Treinamento com dados temporais (ex: sequ√™ncias ou ConvLSTM).
- Uso de infraestrutura com GPU dedicada (Google Colab Pro ou A100).

---

## üíª Interface Interativa

A interface foi desenvolvida com **Gradio** e est√° dispon√≠vel na Hugging Face.

![Interface via Gradio](static/images/interface.png)

üëâ Acesse: [https://huggingface.co/spaces/carolinasoares/urban\_disaster\_monitor](https://huggingface.co/spaces/carolinasoares/urban_disaster_monitor)

---

## üõ†Ô∏è Tecnologias

- [Python 3.10](https://www.python.org/)
- [YOLOv8 (Ultralytics)](https://docs.ultralytics.com)
- [OpenCV](https://opencv.org/)
- [Gradio](https://gradio.app/)
- [Matplotlib](https://matplotlib.org/)

---

## üë• Equipe do Projeto

O desenvolvimento do `Urban Disaster Monitor` √© realizado por alunos da disciplina de Vis√£o Computacional, ministrada pelo professor [Helton Maia](https://heltonmaia.com/) da ECT/UFRN:

| [![](https://github.com/jagaldino.png?size=80)](https://github.com/jagaldino) | [![](https://github.com/MariaCarolinass.png?size=80)](https://github.com/MariaCarolinass) | [![](https://github.com/heltonmaia.png?size=80)](https://github.com/heltonmaia) |
| :---------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------: |
|                           **jagaldino** Pesquisador                           |                             **MariaCarolinass** Pesquisadora                              |                       **heltonmaia** Professor Orientador                       |

---

## üß† Reposit√≥rio de Dados

O dataset anotado e pr√©-processado ser√° disponibilizado para a comunidade cient√≠fica (ou referenciado atrav√©s de um link permanente) ap√≥s a conclus√£o das etapas de anota√ß√£o e valida√ß√£o, aderindo aos princ√≠pios de ci√™ncia aberta e reprodutibilidade.

- **Link Roboflow**: _(a ser adicionado)_
- **Formato das Anota√ß√µes**: YOLOv8 (arquivos `.txt` contendo as coordenadas das _bounding boxes_ e os IDs das classes, acompanhados das imagens correspondentes, seguindo a conven√ß√£o de nomenclatura do YOLO).

---

## üìÅ Estrutura do Projeto

```
urban-disaster-monitor/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ best30p.pt
|   |   ‚îî‚îÄ‚îÄ best50p.pt
‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ dataset/
‚îÇ   ‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ val/
‚îÇ   ‚îú‚îÄ‚îÄ data.yaml/
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ urban_disaster_monitor.ipynb
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ graphics/
‚îÇ   ‚îî‚îÄ‚îÄ matrix/
‚îú‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ train.py
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ LICENSE
```

---

## üöÄ Como Executar

Baixe o reposit√≥rio:

```bash
git clone https://github.com/MariaCarolinass/urban-disaster-monitor.git
cd urban-disaster-monitor/app
```

Crie e ative o ambiente virtual venv:

```bash
python3 -m venv venv
source venv/bin/activate
```

Instale as bibliotecas:

```bash
pip install -r requirements.txt
```

Execute o projeto:

```bash
python app.py
```

---

## üìö Refer√™ncias Bibliogr√°ficas

- Projeto SPICI (Smart Platform for Images and Critical Information). Dispon√≠vel em: [https://smlab.imd.ufrn.br/projeto-spici/](https://smlab.imd.ufrn.br/projeto-spici/)
- Ultralytics. **YOLOv8 Documentation**. Dispon√≠vel em: [https://docs.ultralytics.com](https://docs.ultralytics.com)
- Roboflow. Dispon√≠vel em: [https://roboflow.com](https://roboflow.com)
- Redmon, J.; Farhadi, A. (2018). **YOLOv3: An Incremental Improvement**. _arXiv preprint arXiv:1804.02767_. Dispon√≠vel em: [https://arxiv.org/abs/1804.02767](https://arxiv.org/abs/1804.02767)
- Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. (2016). **You Only Look Once: Unified, Real-Time Object Detection**. In: _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 779-788. Dispon√≠vel em: [https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640)
