# Urban Disaster Monitor: Um Sistema de Detec√ß√£o e Classifica√ß√£o de Indiv√≠duos em Cen√°rios de Desastre Urbano Baseado em Vis√£o Computacional

## üìå Introdu√ß√£o

Eventos de **desastre urbano**, como colapsos estruturais, inunda√ß√µes e deslizamentos de terra, imp√µem desafios significativos √†s opera√ß√µes de resposta e resgate. A capacidade de **identificar e categorizar rapidamente** indiv√≠duos como civis ou socorristas em tempo real √© crucial para a otimiza√ß√£o da aloca√ß√£o de recursos e a minimiza√ß√£o de fatalidades. Imagens coletadas por ve√≠culos a√©reos n√£o tripulados (VANTs), sistemas de vigil√¢ncia e dispositivos m√≥veis representam uma fonte de dados valiosa para esta finalidade.

Este projeto prop√µe o desenvolvimento de um **sistema inteligente de vis√£o computacional** para a **detec√ß√£o e classifica√ß√£o discriminativa de indiv√≠duos (civis e socorristas)** em ambientes impactados por desastres urbanos. Utilizando a arquitetura de **detec√ß√£o de objetos YOLOv8 (You Only Look Once, vers√£o 8)**, o objetivo √© construir uma ferramenta robusta e eficiente capaz de fornecer suporte cr√≠tico a autoridades e equipes de emerg√™ncia durante a fase de resposta a desastres.

Desenvolvido no **SMLab (Smart Metropolis Lab)** do Instituto Metr√≥pole Digital (IMD/UFRN), este trabalho √© parte integrante do **Projeto SPICI (Seguran√ßa P√∫blica Integrada em Cidades Inteligentes)**. O SPICI visa criar uma plataforma inteligente para coleta, processamento e an√°lise de imagens e informa√ß√µes cr√≠ticas em tempo real, contribuindo para a gest√£o de crises e desastres por meio de tecnologias avan√ßadas como vis√£o computacional e intelig√™ncia artificial. O `Urban Disaster Monitor` alinha-se diretamente com os objetivos do SPICI ao fornecer uma ferramenta especializada para a identifica√ß√£o de pessoas em cen√°rios de desastre, agregando valor √† capacidade de resposta e tomada de decis√£o da plataforma.

-----

## ‚öôÔ∏è Metodologia e Materiais

### Aquisi√ß√£o e Anota√ß√£o de Dados

Para o treinamento e valida√ß√£o do modelo, estamos compilando um **dataset abrangente** composto por diversas imagens que representam cen√°rios t√≠picos de desastres urbanos (e.g., edif√≠cios colapsados, √°reas inundadas). As fontes incluem **reposit√≥rios p√∫blicos de imagens de desastres** e a **gera√ß√£o de imagens sint√©ticas** para complementar a variabilidade do conjunto de dados. Essa abordagem h√≠brida visa mitigar a escassez de dados reais de alta qualidade em ambientes de desastre.

A **anota√ß√£o dos objetos de interesse** est√° sendo realizada na plataforma **Roboflow**, onde as classes prim√°rias de interesse s√£o definidas como:

  - `People` (Civis): Indiv√≠duos n√£o identificados como membros de equipes de resgate.
  - `Rescuer` (Socorristas): Indiv√≠duos equipados ou identific√°veis como parte de equipes de emerg√™ncia (e.g., bombeiros, param√©dicos), frequentemente distingu√≠veis por uniformes, equipamentos de prote√ß√£o individual (EPI) ou posturas operacionais.

### Pr√©-processamento de Dados

As etapas de pr√©-processamento s√£o cruciais para otimizar o desempenho do modelo e garantir sua generaliza√ß√£o:

  - **Redimensionamento**: Todas as imagens s√£o escaladas para dimens√µes compat√≠veis com as entradas da arquitetura YOLOv8, balanceando a fidelidade da informa√ß√£o visual com a efici√™ncia computacional.
  - **Aumento de Dados (Data Augmentation)**: T√©cnicas de aumento de dados (e.g., rota√ß√£o, transla√ß√£o, espelhamento, ajuste de brilho e contraste, *blurring* e ru√≠do) s√£o aplicadas para aumentar a robustez do modelo, mitigar o risco de *overfitting* e compensar potenciais desequil√≠brios entre as classes. A diversifica√ß√£o do dataset atrav√©s do *data augmentation* √© fundamental para simular a variabilidade das condi√ß√µes de imagem em cen√°rios reais de desastre.
  - **Particionamento**: O dataset √© dividido em conjuntos de treino, valida√ß√£o e teste, tipicamente nas propor√ß√µes 70%, 20% e 10%, respectivamente. Essa divis√£o estratificada assegura uma avalia√ß√£o imparcial da capacidade de generaliza√ß√£o do modelo para dados n√£o vistos.

### Arquitetura do Modelo e Ferramentas

  - **Modelo de Detec√ß√£o**: **YOLOv8 (You Only Look Once, vers√£o 8)**. Essa arquitetura foi selecionada por sua comprovada efici√™ncia na detec√ß√£o de objetos em tempo real, combinando alta acur√°cia com velocidade de infer√™ncia, caracter√≠sticas essenciais para aplica√ß√µes em cen√°rios emergenciais.
  - **Tarefa**: Detec√ß√£o de Objetos (Object Detection).
  - **Frameworks e Bibliotecas**: O desenvolvimento e treinamento do modelo s√£o realizados com **Ultralytics YOLOv8**, utilizando **PyTorch** como *backend* de aprendizado profundo, e **OpenCV** para manipula√ß√£o de imagens e pr√©-processamento de v√≠deo.

### Treinamento e Otimiza√ß√£o do Modelo

O processo de treinamento envolve as seguintes fases, visando a otimiza√ß√£o do desempenho do modelo:

1.  **Convers√£o de Anota√ß√µes**: As anota√ß√µes geradas no formato Roboflow s√£o convertidas para o formato YOLO (`.txt`), que √© o padr√£o de entrada para os modelos da fam√≠lia YOLO.
2.  **Treinamento Iterativo**: O modelo √© treinado utilizando um conjunto de par√¢metros otimizados, incluindo *batch size*, taxa de aprendizado (*learning rate*) e n√∫mero de *epochs*. T√©cnicas de agendamento da taxa de aprendizado (*learning rate schedulers*) e otimizadores (e.g., Adam, SGD) s√£o empregadas para acelerar a converg√™ncia e melhorar a performance.
3.  **Valida√ß√£o Cont√≠nua**: O desempenho do modelo √© monitorado e validado em tempo real durante o treinamento utilizando m√©tricas de desempenho padr√£o da √°rea em um conjunto de valida√ß√£o separado. Isso permite identificar *overfitting* e ajustar hiperpar√¢metros de forma din√¢mica.
4.  **Testes e Avalia√ß√£o**: Ap√≥s o treinamento, o modelo √© avaliado extensivamente em um conjunto de teste independente, contendo imagens reais e cen√°rios n√£o vistos, para verificar sua capacidade de generaliza√ß√£o e robustez em condi√ß√µes adversas.

-----

## üìä An√°lise de Resultados e M√©tricas

A avalia√ß√£o da efic√°cia do modelo ser√° realizada atrav√©s de um conjunto de m√©tricas quantitativas e qualitativas, fornecendo uma an√°lise abrangente do seu desempenho:

  - **M√©tricas de Precis√£o M√©dia (mAP)**:
      - **mAP@0.5**: *Mean Average Precision* calculada com um *IoU (Intersection over Union)* *threshold* de 0.5. Essa m√©trica √© comumente utilizada para avalia√ß√£o r√°pida da precis√£o de detec√ß√£o.
      - **mAP@0.5:0.95**: *Mean Average Precision* calculada em m√∫ltiplos *thresholds* de *IoU*, variando de 0.5 a 0.95 em etapas de 0.05. Essa m√©trica oferece uma medida mais robusta e abrangente da acur√°cia de localiza√ß√£o e classifica√ß√£o das detec√ß√µes.
  - **Precis√£o (Precision) e Recall (Revoca√ß√£o)**: Avaliados por classe para compreender o desempenho individual na identifica√ß√£o de civis e socorristas, indicando a propor√ß√£o de detec√ß√µes corretas e a capacidade do modelo de encontrar todas as inst√¢ncias relevantes, respectivamente.
  - **Matriz de Confus√£o (Confusion Matrix)**: Essencial para visualizar e analisar os tipos de erros (falsos positivos, falsos negativos) cometidos pelo modelo, especialmente a confus√£o entre as classes de interesse, o que √© cr√≠tico em cen√°rios onde a distin√ß√£o entre civis e socorristas √© vital.
  - **An√°lise Qualitativa**: Ser√£o apresentadas visualiza√ß√µes das *bounding boxes* e r√≥tulos de classe sobre as imagens de teste. Essa an√°lise visual permite uma avalia√ß√£o subjetiva da acur√°cia das detec√ß√µes e da capacidade do modelo de lidar com varia√ß√µes de escala, oclus√£o e ilumina√ß√£o.

Os resultados ser√£o detalhadamente apresentados em relat√≥rios t√©cnicos e artigos cient√≠ficos, incluindo gr√°ficos e tabelas estat√≠sticas, acompanhados de uma an√°lise cr√≠tica sobre as limita√ß√µes do modelo e o potencial de aplica√ß√£o em contextos reais de desastre.

-----

## üîç Discuss√£o e Conclus√µes (a ser preenchido ap√≥s a experimenta√ß√£o)

*(Esta se√ß√£o ser√° dedicada √† discuss√£o aprofundada dos resultados obtidos, incluindo a interpreta√ß√£o das m√©tricas de desempenho, a identifica√ß√£o de pontos fortes e fracos do modelo, e a an√°lise de poss√≠veis vieses nos dados. Ser√£o abordadas as limita√ß√µes inerentes √† metodologia, como a sensibilidade a condi√ß√µes de ilumina√ß√£o extremas, oclus√£o parcial ou total dos indiv√≠duos, e a representatividade do dataset. Al√©m disso, ser√£o propostas futuras linhas de pesquisa para aprimoramento do sistema, como a integra√ß√£o de informa√ß√µes contextuais (e.g., sensores multiespectrais), a otimiza√ß√£o para implanta√ß√£o em dispositivos de borda para infer√™ncia em tempo real e a explora√ß√£o de arquiteturas de modelos mais leves. Finalmente, ser√° discutida a integra√ß√£o do `Urban Disaster Monitor` como um m√≥dulo crucial dentro da plataforma SPICI, real√ßando seu impacto potencial na gest√£o inteligente de desastres e na coordena√ß√£o de equipes de resgate.)*

-----

## üë• Equipe do Projeto

O desenvolvimento do `Urban Disaster Monitor` √© realizado por alunos da mat√©ria de Vis√£o Computacional, dada pelo professor **Helton Maia** da ECT/UFRN:

| Membro              | Papel               |
| :------------------ | :------------------ |
| **jagaldino** | Pesquisador(a)      |
| **MariaCarolinass** | Pesquisador(a)      |
| **heltonmaia** | Professor Orientador |

-----

## üíª Instru√ß√µes para Reprodu√ß√£o Local

Para replicar o ambiente de desenvolvimento e executar o modelo localmente, siga os passos abaixo. Certifique-se de que o sistema atenda aos requisitos de hardware para treinamento de modelos de *deep learning* (e.g., GPU com CUDA).

```bash
# 1. Clonar o reposit√≥rio do projeto
git clone https://github.com/seu-usuario/urban-disaster-monitor.git
cd urban-disaster-monitor

# 2. Instalar as depend√™ncias do projeto (requer Python 3.8+ e pip)
# √â altamente recomendado o uso de um ambiente virtual (e.g., conda ou venv)
pip install -r requirements.txt

# 3. Exemplo de comando para treinamento do modelo utilizando a CLI da Ultralytics
# Certifique-se de que o arquivo 'dataset.yaml' (com as configura√ß√µes do seu dataset)
# e o modelo base 'yolov8n.pt' (ou outra variante YOLOv8) estejam configurados corretamente.
yolo task=detect mode=train model=yolov8n.pt data=dataset.yaml epochs=50 imgsz=640
```

-----

## üß† Reposit√≥rio de Dados

O dataset anotado e pr√©-processado ser√° disponibilizado para a comunidade cient√≠fica (ou referenciado atrav√©s de um link permanente) ap√≥s a conclus√£o das etapas de anota√ß√£o e valida√ß√£o, aderindo aos princ√≠pios de ci√™ncia aberta e reprodutibilidade.

  - **Link Roboflow**: *(a ser adicionado)*
  - **Formato das Anota√ß√µes**: YOLOv8 (arquivos `.txt` contendo as coordenadas das *bounding boxes* e os IDs das classes, acompanhados das imagens correspondentes, seguindo a conven√ß√£o de nomenclatura do YOLO).

-----

## üìÅ Estrutura do Projeto

```
EM CONSTRU√á√ÉO
```

-----

## üìö Refer√™ncias Bibliogr√°ficas

  - Projeto SPICI (Smart Platform for Images and Critical Information): [https://smlab.imd.ufrn.br/projeto-spici/](https://smlab.imd.ufrn.br/projeto-spici/)
  - Ultralytics YOLOv8 Docs: [https://docs.ultralytics.com](https://docs.ultralytics.com)
  - Roboflow: [https://roboflow.com](https://roboflow.com)
  - Redmon, J., Farhadi, A. (2018). **YOLOv3: An Incremental Improvement**. *arXiv preprint arXiv:1804.02767*.
  - Redmon, J., Divvala, S., Girshick, R., Farhadi, A. (2016). **You Only Look Once: Unified, Real-Time Object Detection**. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, pp. 779-788.
  - Dataset p√∫blicos: [Google Open Images](https://storage.googleapis.com/openimages/web/index.html), [Kaggle Datasets](https://www.kaggle.com/datasets)
