# Urban Disaster Monitor: Um Sistema de Detec√ß√£o e Classifica√ß√£o de Indiv√≠duos em Cen√°rios de Desastre Urbano Baseado em Vis√£o Computacional

## üìå Introdu√ß√£o

Eventos de **desastre urbano**, como colapsos estruturais, inunda√ß√µes e deslizamentos de terra, imp√µem desafios significativos √†s opera√ß√µes de resposta e resgate. A capacidade de **identificar e categorizar rapidamente** indiv√≠duos como civis ou socorristas em tempo real √© crucial para a otimiza√ß√£o da aloca√ß√£o de recursos e a minimiza√ß√£o de fatalidades. Imagens coletadas por ve√≠culos a√©reos n√£o tripulados (VANTs), sistemas de vigil√¢ncia e dispositivos m√≥veis representam uma fonte de dados valiosa para esta finalidade.

Este projeto prop√µe o desenvolvimento de um **sistema inteligente de vis√£o computacional** para a **detec√ß√£o e classifica√ß√£o discriminativa de indiv√≠duos (civis e socorristas)** em ambientes impactados por desastres urbanos. Utilizando a arquitetura de **detec√ß√£o de objetos YOLOv8 (You Only Look Once, vers√£o 8)**, o objetivo √© construir uma ferramenta robusta e eficiente capaz de fornecer suporte cr√≠tico a autoridades e equipes de emerg√™ncia durante a fase de resposta a desastres.

Desenvolvido no **SMLab (Smart Metropolis Lab)** do Instituto Metr√≥pole Digital (IMD/UFRN), este trabalho √© parte integrante do **Projeto SPICI (Seguran√ßa P√∫blica Integrada em Cidades Inteligentes)**. O SPICI visa criar uma plataforma inteligente para coleta, processamento e an√°lise de imagens e informa√ß√µes cr√≠ticas em tempo real, contribuindo para a gest√£o de crises e desastres por meio de tecnologias avan√ßadas como vis√£o computacional e intelig√™ncia artificial. O `Urban Disaster Monitor` alinha-se diretamente com os objetivos do SPICI ao fornecer uma ferramenta especializada para a identifica√ß√£o de pessoas em cen√°rios de desastre, agregando valor √† capacidade de resposta e tomada de decis√£o da plataforma.

---

## ‚öôÔ∏è Metodologia e Materiais

### Aquisi√ß√£o e Anota√ß√£o de Dados

Para o treinamento e valida√ß√£o do modelo, estamos compilando um **dataset abrangente** composto por diversas imagens que representam cen√°rios t√≠picos de desastres urbanos (e.g., edif√≠cios colapsados, √°reas inundadas). As fontes incluem **reposit√≥rios p√∫blicos de imagens de desastres** e a **gera√ß√£o de imagens sint√©ticas** para complementar a variabilidade do conjunto de dados. Essa abordagem h√≠brida visa mitigar a escassez de dados reais de alta qualidade em ambientes de desastre.

A **anota√ß√£o dos objetos de interesse** est√° sendo realizada na plataforma **Roboflow**, onde as classes prim√°rias de interesse s√£o definidas como:

- `People` (Civis): Indiv√≠duos n√£o identificados como membros de equipes de resgate.
- `Rescuer` (Socorristas): Indiv√≠duos equipados ou identific√°veis como parte de equipes de emerg√™ncia (e.g., bombeiros, param√©dicos), frequentemente distingu√≠veis por uniformes, equipamentos de prote√ß√£o individual (EPI) ou posturas operacionais.

### Pr√©-processamento de Dados

As etapas de pr√©-processamento s√£o cruciais para otimizar o desempenho do modelo e garantir sua generaliza√ß√£o:

- **Redimensionamento**: Todas as imagens s√£o escaladas para dimens√µes compat√≠veis com as entradas da arquitetura YOLOv8, balanceando a fidelidade da informa√ß√£o visual com a efici√™ncia computacional.
- **Aumento de Dados (Data Augmentation)**: T√©cnicas de aumento de dados (e.g., rota√ß√£o, transla√ß√£o, espelhamento, ajuste de brilho e contraste, _blurring_ e ru√≠do) s√£o aplicadas para aumentar a robustez do modelo, mitigar o risco de _overfitting_ e compensar potenciais desequil√≠brios entre as classes. A diversifica√ß√£o do dataset atrav√©s do _data augmentation_ √© fundamental para simular a variabilidade das condi√ß√µes de imagem em cen√°rios reais de desastre.
- **Particionamento**: O dataset √© dividido em conjuntos de treino, valida√ß√£o e teste, tipicamente nas propor√ß√µes 70%, 20% e 10%, respectivamente. Essa divis√£o estratificada assegura uma avalia√ß√£o imparcial da capacidade de generaliza√ß√£o do modelo para dados n√£o vistos.

### Arquitetura do Modelo e Ferramentas

- **Modelo de Detec√ß√£o**: **YOLOv8 (You Only Look Once, vers√£o 8)**. Essa arquitetura foi selecionada por sua comprovada efici√™ncia na detec√ß√£o de objetos em tempo real, combinando alta acur√°cia com velocidade de infer√™ncia, caracter√≠sticas essenciais para aplica√ß√µes em cen√°rios emergenciais.
- **Tarefa**: Detec√ß√£o de Objetos (Object Detection).
- **Frameworks e Bibliotecas**: O desenvolvimento e treinamento do modelo s√£o realizados com **Ultralytics YOLOv8**, utilizando **PyTorch** como _backend_ de aprendizado profundo, e **OpenCV** para manipula√ß√£o de imagens e pr√©-processamento de v√≠deo.

### Treinamento e Otimiza√ß√£o do Modelo

O processo de treinamento envolve as seguintes fases, visando a otimiza√ß√£o do desempenho do modelo:

1.  **Convers√£o de Anota√ß√µes**: As anota√ß√µes geradas no formato Roboflow s√£o convertidas para o formato YOLO (`.txt`), que √© o padr√£o de entrada para os modelos da fam√≠lia YOLO.
2.  **Treinamento Iterativo**: O modelo √© treinado utilizando um conjunto de par√¢metros otimizados, incluindo _batch size_, taxa de aprendizado (_learning rate_) e n√∫mero de _epochs_. T√©cnicas de agendamento da taxa de aprendizado (_learning rate schedulers_) e otimizadores (e.g., Adam, SGD) s√£o empregadas para acelerar a converg√™ncia e melhorar a performance.
3.  **Valida√ß√£o Cont√≠nua**: O desempenho do modelo √© monitorado e validado em tempo real durante o treinamento utilizando m√©tricas de desempenho padr√£o da √°rea em um conjunto de valida√ß√£o separado. Isso permite identificar _overfitting_ e ajustar hiperpar√¢metros de forma din√¢mica.
4.  **Testes e Avalia√ß√£o**: Ap√≥s o treinamento, o modelo √© avaliado extensivamente em um conjunto de teste independente, contendo imagens reais e cen√°rios n√£o vistos, para verificar sua capacidade de generaliza√ß√£o e robustez em condi√ß√µes adversas.

---

## üìä An√°lise de Resultados e M√©tricas

A avalia√ß√£o da efic√°cia do modelo ser√° realizada atrav√©s de um conjunto de m√©tricas quantitativas e qualitativas, fornecendo uma an√°lise abrangente do seu desempenho:

- **M√©tricas de Precis√£o M√©dia (mAP)**:
  - **mAP@0.5**: _Mean Average Precision_ calculada com um _IoU (Intersection over Union)_ _threshold_ de 0.5. Essa m√©trica √© comumente utilizada para avalia√ß√£o r√°pida da precis√£o de detec√ß√£o.
  - **mAP@0.5:0.95**: _Mean Average Precision_ calculada em m√∫ltiplos _thresholds_ de _IoU_, variando de 0.5 a 0.95 em etapas de 0.05. Essa m√©trica oferece uma medida mais robusta e abrangente da acur√°cia de localiza√ß√£o e classifica√ß√£o das detec√ß√µes.
- **Precis√£o (Precision) e Recall (Revoca√ß√£o)**: Avaliados por classe para compreender o desempenho individual na identifica√ß√£o de civis e socorristas, indicando a propor√ß√£o de detec√ß√µes corretas e a capacidade do modelo de encontrar todas as inst√¢ncias relevantes, respectivamente.
- **Matriz de Confus√£o (Confusion Matrix)**: Essencial para visualizar e analisar os tipos de erros (falsos positivos, falsos negativos) cometidos pelo modelo, especialmente a confus√£o entre as classes de interesse, o que √© cr√≠tico em cen√°rios onde a distin√ß√£o entre civis e socorristas √© vital.
- **An√°lise Qualitativa**: Ser√£o apresentadas visualiza√ß√µes das _bounding boxes_ e r√≥tulos de classe sobre as imagens de teste. Essa an√°lise visual permite uma avalia√ß√£o subjetiva da acur√°cia das detec√ß√µes e da capacidade do modelo de lidar com varia√ß√µes de escala, oclus√£o e ilumina√ß√£o.

Os resultados ser√£o detalhadamente apresentados em relat√≥rios t√©cnicos e artigos cient√≠ficos, incluindo gr√°ficos e tabelas estat√≠sticas, acompanhados de uma an√°lise cr√≠tica sobre as limita√ß√µes do modelo e o potencial de aplica√ß√£o em contextos reais de desastre.

---

# üìà Resultados Experimentais

## üì∑ Avalia√ß√£o em Imagens Est√°ticas

Foram realizados testes qualitativos em imagens p√∫blicas de alagamentos para validar a capacidade de detec√ß√£o do modelo YOLOv8 treinado em diferentes regimes de √©pocas (30 e 50). As imagens utilizadas foram:

[BBC Brasil - Alagamentos em SP](https://www.bbc.com/portuguese/articles/cw00d51k5rlo)

[UOL Not√≠cias - Jd. Pantanal ap√≥s tempestade](https://noticias.uol.com.br/cotidiano/ultimas-noticias/2025/02/03/alagamentos-de-3-dias-imagens-mostram-jd-pantanal-apos-tempestade-em-sp.htm)

üìå **Resultado:**

Com **30 √©pocas**, o modelo j√° era capaz de detectar pessoas e socorristas com precis√£o razo√°vel.

Com **50 √©pocas**, observou-se uma melhora clara nas imagens est√°ticas, com menos falsos positivos, corrigindo erros como a identifica√ß√£o incorreta de pessoas em postes ou √°reas de sombra.

## üéûÔ∏è Avalia√ß√£o em V√≠deo

Um v√≠deo p√∫blico do YouTube foi utilizado para simular um cen√°rio real de desastre urbano:

[V√≠deo utilizado no experimento](https://www.youtube.com/watch?v=QnFwDqzCwRU)

üìå **Resultado:**

O modelo com **30 √©pocas** apresentou maior estabilidade e consist√™ncia ao longo dos quadros.

O modelo com **50 √©pocas**, embora superior para imagens, teve comportamento err√°tico em v√≠deo, gerando detec√ß√µes flutuantes e menos confi√°veis ‚Äî sugerindo poss√≠vel overfitting ou limita√ß√£o da generaliza√ß√£o temporal.

üß† **Observa√ß√£o T√©cnica**:
Treinar com mais de 50 √©pocas pode exigir hardware com maior capacidade de mem√≥ria. Durante o experimento, o uso da conta acad√™mica no Google Colab atingiu o limite de mem√≥ria, refor√ßando a necessidade de infraestrutura mais robusta para lidar com sequ√™ncias temporais (v√≠deos).

## üìä Gr√°ficos e Visualiza√ß√µes

Abaixo ser√£o inseridos os gr√°ficos comparativos:

Precis√£o por n√∫mero de √©pocas (30 vs 50)

Performance por tipo de m√≠dia (imagem vs v√≠deo)

Matriz de confus√£o por classe

Bounding boxes qualitativas em imagens reais

‚è≥ _Gr√°ficos em constru√ß√£o ‚Äî ser√£o inseridos at√© a finaliza√ß√£o do relat√≥rio._

## ‚úÖ Conclus√µes Parciais

**30 √©pocas** ‚Üí melhor desempenho em v√≠deos, com menos ru√≠do nas detec√ß√µes.

**50 √©pocas** ‚Üí superioridade em imagens est√°ticas, com maior precis√£o espacial.

üîÅ **Melhorias futuras sugeridas**

Ajuste de hiperpar√¢metros espec√≠ficos para v√≠deo.

Treinamento com dados temporais (ex: sequ√™ncias ou ConvLSTM).

Uso de infraestrutura com GPU dedicada (Google Colab Pro ou A100).

## üîç Discuss√£o e Conclus√µes (a ser preenchido ap√≥s a experimenta√ß√£o)

_(Esta se√ß√£o ser√° dedicada √† discuss√£o aprofundada dos resultados obtidos, incluindo a interpreta√ß√£o das m√©tricas de desempenho, a identifica√ß√£o de pontos fortes e fracos do modelo, e a an√°lise de poss√≠veis vieses nos dados. Ser√£o abordadas as limita√ß√µes inerentes √† metodologia, como a sensibilidade a condi√ß√µes de ilumina√ß√£o extremas, oclus√£o parcial ou total dos indiv√≠duos, e a representatividade do dataset. Al√©m disso, ser√£o propostas futuras linhas de pesquisa para aprimoramento do sistema, como a integra√ß√£o de informa√ß√µes contextuais (e.g., sensores multiespectrais), a otimiza√ß√£o para implanta√ß√£o em dispositivos de borda para infer√™ncia em tempo real e a explora√ß√£o de arquiteturas de modelos mais leves. Finalmente, ser√° discutida a integra√ß√£o do `Urban Disaster Monitor` como um m√≥dulo crucial dentro da plataforma SPICI, real√ßando seu impacto potencial na gest√£o inteligente de desastres e na coordena√ß√£o de equipes de resgate.)_

---

## üë• Equipe do Projeto

O desenvolvimento do `Urban Disaster Monitor` √© realizado por alunos da disciplina de Vis√£o Computacional, ministrada pelo professor **Helton Maia** da ECT/UFRN:

| [![](https://github.com/jagaldino.png?size=80)](https://github.com/jagaldino) | [![](https://github.com/MariaCarolinass.png?size=80)](https://github.com/MariaCarolinass) | [![](https://github.com/heltonmaia.png?size=80)](https://github.com/heltonmaia) |
| :---------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------: |
|                           **jagaldino** Pesquisador                           |                             **MariaCarolinass** Pesquisadora                              |                       **heltonmaia** Professor Orientador                       |

## üíª Instru√ß√µes para Reprodu√ß√£o Local

```
EM CONSTRU√á√ÉO
```

---

## üß† Reposit√≥rio de Dados

O dataset anotado e pr√©-processado ser√° disponibilizado para a comunidade cient√≠fica (ou referenciado atrav√©s de um link permanente) ap√≥s a conclus√£o das etapas de anota√ß√£o e valida√ß√£o, aderindo aos princ√≠pios de ci√™ncia aberta e reprodutibilidade.

- **Link Roboflow**: _(a ser adicionado)_
- **Formato das Anota√ß√µes**: YOLOv8 (arquivos `.txt` contendo as coordenadas das _bounding boxes_ e os IDs das classes, acompanhados das imagens correspondentes, seguindo a conven√ß√£o de nomenclatura do YOLO).

---

## üìÅ Estrutura do Projeto

```
EM CONSTRU√á√ÉO
```

---

## üìö Refer√™ncias Bibliogr√°ficas

- Projeto SPICI (Smart Platform for Images and Critical Information): [https://smlab.imd.ufrn.br/projeto-spici/](https://smlab.imd.ufrn.br/projeto-spici/)
- Ultralytics YOLOv8 Docs: [https://docs.ultralytics.com](https://docs.ultralytics.com)
- Roboflow: [https://roboflow.com](https://roboflow.com)
- Redmon, J., Farhadi, A. (2018). **YOLOv3: An Incremental Improvement**. _arXiv preprint arXiv:1804.02767_.
- Redmon, J., Divvala, S., Girshick, R., Farhadi, A. (2016). **You Only Look Once: Unified, Real-Time Object Detection**. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 779-788.
- Dataset p√∫blicos: [Google Open Images](https://storage.googleapis.com/openimages/web/index.html), [Kaggle Datasets](https://www.kaggle.com/datasets)
